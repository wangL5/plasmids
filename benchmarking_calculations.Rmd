---
title: "Plasmids Benchmarking"
output: html_document
date: "2025-04-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Read in file and load in libraries
```{r}
library(tidyverse)
library(janitor)

CAMI2_benchmark_data <- read_excel("dataset_processing_notes.xlsx", sheet = "CAMI2_benchmark")
```

### Look at dataset
```{r}
head(CAMI2_benchmark_data)
```

### clean up column names
```{r}
CAMI2_data_clean <- CAMI2_benchmark_data %>%
  clean_names()

head(CAMI2_data_clean)

```

### check for missing values
```{r}
summary(CAMI2_data_clean)
```
### reshape the data
```{r}
CAMI2_long <- CAMI2_data_clean %>% 
  pivot_longer(cols = -sample,  # pivot everything excempt sample column 
               names_to = c("classifier", "assembler", "metric"),  # what we're changing column names to 
               names_pattern = "(.*)_(.*)_(.*)",  # names separated by _ 
               values_to = "value")
head(CAMI2_long)
```
## save cleaned up long file
```{r}
library(readr)
write_tsv(CAMI2_long, "CAMI2_benchmark_data_long.tsv")
```

## calculate benchmarking metrics 
```{r}
library(dplyr)
library(tidyr)

CAMI2_benchmarking_metrics <- CAMI2_long %>%
  # pivot again to get metrics as columns 
  pivot_wider(names_from = metric, values_from = value) %>%
  mutate(
    accuracy = (tp + tn) / (tp + tn + fp + fn),
    precision = tp / (tp + fp),
    recall = tp / (tp + fn),
    specificity = tn / (tn + fp),
    false_positive_rate = fp / (tn + fp),
    f1_score = 2 * (precision * recall) / (precision + recall)
  )
```

### view table
```{r}
print(CAMI2_benchmarking_metrics)
```


### save table
```{r}
write_tsv(CAMI2_benchmarking_metrics, "CAMI2_benchmarking_metrics.tsv")
```

### check for NaN
```{r}
library(dplyr)
nan_rows <- CAMI2_benchmarking_metrics %>%
  filter(if_any(everything(), is.nan))

nan_rows

## plasmidfinder data has some NaN/0 data
```

### check for precision/recall values
```{r}
nan_rows2 <- CAMI2_benchmarking_metrics %>% 
  filter(is.nan(precision) | is.nan(recall))

nan_rows2

# two rows here have NaN for precision, and 0 for recall. Both are from plasmidfinder data.
```



## precision-recall
```{r}
library(ggplot2)
library(dplyr)
library(pals)

# precision-recall curve
CAMI2_PR_curve <- ggplot(CAMI2_benchmarking_metrics, aes(x = recall, y = precision, color = classifier)) + scale_color_manual(values=unname(alphabet())) +
  geom_point(alpha = 0.7, size = 2) +
  facet_wrap(~ assembler) +
  labs(
    title = "Precision vs Recall by Assembler and Classifier",
    x = "Recall",
    y = "Precision",
    color = "Classifier"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold"),
    legend.position = "bottom",
    plot.title = element_text(hjust=0.5)
  )

```

```{r}
plot(CAMI2_PR_curve)
```

### get summary metrics (mean + sd) of benchmarking parameters

```{r}
library(dplyr)

CAMI2_summary_metrics <- CAMI2_benchmarking_metrics %>%
  group_by(classifier, assembler) %>%
  summarise(
    n = n(),
    mean_accuracy = mean(accuracy, na.rm = TRUE),
    sd_accuracy = sd(accuracy, na.rm = TRUE),
    
    mean_precision = mean(precision, na.rm = TRUE),
    sd_precision = sd(precision, na.rm = TRUE),
    
    mean_recall = mean(recall, na.rm = TRUE),
    sd_recall = sd(recall, na.rm = TRUE),
    
    mean_specificity = mean(specificity, na.rm = TRUE),
    sd_specificity = sd(specificity, na.rm = TRUE),
    
    mean_fpr = mean(false_positive_rate, na.rm = TRUE),
    sd_fpr = sd(false_positive_rate, na.rm = TRUE),
    
    mean_f1_score = mean(f1_score, na.rm = TRUE),
    sd_f1_score = sd(f1_score, na.rm = TRUE),
  )

print(CAMI2_summary_metrics)

```
### save summary table
```{r}
write_tsv(CAMI2_summary_metrics, "CAMI2_benchmark_summary_metrics.tsv")
```

### save figure
```{r}
ggsave("CAMI2_precision_recall.png", plot = CAMI2_PR_curve, bg = 'white', width = 10, height = 6, dpi = 300)

```

### view genomad and plasx summary benchmark metrics
```{r}
library(dplyr)

selected_means <- CAMI2_summary_metrics %>%
  filter(classifier %in% c("genomad", "plasx075", "plasx09"),
         assembler %in% c("megahit", "mpspades")) %>%
  select(classifier, assembler, mean_precision, mean_recall)

print(selected_means)

```

## box-plots of benchmarking parameters

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)

# Assuming your metric data is in `metrics_df` with columns:
# assembler, classifier, sample, accuracy, precision, recall, specificity, false_positive_rate, f1_score

# Pivot metrics into long format for easier plotting
CAMI2_benchmarking_bar_plot_df <- CAMI2_benchmarking_metrics %>%
  pivot_longer(
    cols = c(accuracy, precision, recall, specificity, false_positive_rate, f1_score),
    names_to = "metric",
    values_to = "value"
  )
```

```{r}
head(CAMI2_benchmarking_bar_plot_df)
```

### boxplots of benchmarking parameters
```{r}
# Plot boxplots by assembler and classifier
CAMI2_boxplot <- ggplot(CAMI2_benchmarking_bar_plot_df, aes(x = classifier, y = value, fill = assembler)) +
  scale_fill_manual(values=c("orange", "cornflowerblue")) + 
  geom_boxplot(outlier.shape = NA, alpha = 0.8, position = position_dodge(width = 0.8)) +
  facet_wrap(~metric, scales = "free_y", ncol = 2) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Performance Metrics by Classifier and Assembler", x = "", y = "", fill = "",
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(hjust=0.5)
  )
```

### plot boxplots
```{r}
plot(CAMI2_boxplot)
```
### save boxplots
```{r}
ggsave("CAMI2_metric_boxplot.png", plot = CAMI2_boxplot, bg = 'white', width = 12, height = 8, dpi = 300)
```

### just f1 score box plot
```{r}
f1_score_boxplot <- ggplot(CAMI2_benchmarking_metrics, aes(x = classifier, y = f1_score, fill = assembler)) + geom_boxplot(position = position_dodge(width=0.8)) + 
  scale_fill_manual(values=c("orange", "cornflowerblue")) + 
  theme_minimal(base_size = 14) +
  labs(title = "F1 Score by Classifier and Assembler", x = "", y = "", fill = "",
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(hjust=0.5),
    panel.grid.major.x = element_blank()
  )

```
### save F-1 score boxplot
```{r}
ggsave("CAMI2_f1score_boxplot.png", plot = f1_score_boxplot, bg = 'white', width = 6, height = 4, dpi = 300)
```

### f-1 score ridge plot
```{r}
library(ggridges)

ggplot(CAMI2_benchmarking_metrics, aes(x = f1_score, y = interaction(classifier, assembler), fill = assembler)) +
  geom_density_ridges(alpha = 0.8) +
  theme_minimal(base_size = 14) +
  labs(x= "", y = "", title = "F1 Score Distribution")

```
### bar plots

```{r}
### use table that was summarized by classifier and assembler, with mean and sd
### CAMI2_summary_metrics

# Convert to long format for plotting
long_summary <- CAMI2_summary_metrics %>%
  pivot_longer(
    cols = starts_with("mean_"),
    names_to = "metric",
    values_to = "mean"
  ) %>%
  mutate(
    sd = case_when(
      metric == "mean_accuracy" ~ sd_accuracy,
      metric == "mean_precision" ~ sd_precision,
      metric == "mean_recall" ~ sd_recall,
      metric == "mean_specificity" ~ sd_specificity,
      metric == "mean_fpr" ~ sd_fpr,
      metric == "mean_f1_score" ~ sd_f1_score,
      TRUE ~ NA_real_
    ),
    metric = gsub("mean_", "", metric)
  )
```

```{r}
# Plot bar graph with error bars
ggplot(long_summary, aes(x = interaction(classifier, assembler, sep = " + "), y = mean, fill = assembler)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),
                position = position_dodge(width = 0.8),
                width = 0.25) +
  facet_wrap(~metric, scales = "free_y") +
  labs(x = "Classifier + Assembler", y = "Mean Metric Value", title = "Benchmarking Metrics (Bar Graph)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


### radar plots

```{r}
library(dplyr)
library(tidyr)
library(fmsb)
library(tibble)

# not sure radar would be a good idea because there are so many combinations 

# Summarize metrics
radar_data <- CAMI2_benchmarking_metrics %>%
  group_by(classifier, assembler) %>%
  summarise(across(c(accuracy, precision, recall, specificity, false_positive_rate, f1_score), mean, .names = "mean_{.col}"), .groups = "drop")

# For radar plot, rows = groups, columns = metrics
radar_df <- radar_data %>%
  unite("combo", classifier, assembler, sep = "_") %>%
  column_to_rownames("combo") %>%
  select(starts_with("mean_"))

# fmsb needs a max and min row
radar_df <- rbind(
  rep(1, ncol(radar_df)),  # max values
  rep(0, ncol(radar_df)),  # min values
  radar_df
)

# Plot example (just showing first 3 combos for clarity)
radarchart(radar_df[1:5, ],
           axistype = 1,
           pcol = c("red", "blue", "green"),
           plwd = 2,
           plty = 1,
           cglcol = "grey", 
           cglty = 1,
           axislabcol = "grey",
           vlcex = 0.8)
legend("topright", legend = rownames(radar_df)[3:5], col = c("red", "blue", "green"), lty = 1, lwd = 2)

```

